{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xnQ1kK61Frqu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd5_mbySF2W6",
        "outputId": "4beb7432-6055-42e6-af8a-9726c2317993"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r4Rm0RQmFrq4"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/creditcard.csv')\n",
        "\n",
        "# Split data into fraud and legitimate\n",
        "fraud_data = df[df['Class'] == 1]\n",
        "legit_data = df[df['Class'] == 0]\n",
        "\n",
        "# Split both classes into train and test (80/20)\n",
        "fraud_train, fraud_test = train_test_split(fraud_data, test_size=0.3, random_state=42)\n",
        "legit_train, legit_test = train_test_split(legit_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Preserve test set for final evaluation\n",
        "final_test_set = pd.concat([fraud_test, legit_test])\n",
        "\n",
        "# GAN will only train on fraud TRAINING data\n",
        "fraud_train_features = fraud_data.drop(columns=['Class']).values\n",
        "\n",
        "# Normalize data using scaler fitted ONLY on fraud training data\n",
        "scaler = MinMaxScaler()\n",
        "fraud_train_scaled = scaler.fit_transform(fraud_train_features)\n",
        "\n",
        "# Convert to PyTorch dataset\n",
        "real_data = torch.tensor(fraud_train_scaled, dtype=torch.float32)\n",
        "dataset = TensorDataset(real_data)\n",
        "data_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a2IjWjcyFrq7"
      },
      "outputs": [],
      "source": [
        "# Improved Generator with regularization\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, 512),  # Increased from 256\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, 1024),  # New layer\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Critic (Discriminator) for WGAN-GP\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oyt0H0XZFrq-"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "input_dim = real_data.shape[1]\n",
        "# Adjust training parameters\n",
        "z_dim = 50                        # Increase from 30\n",
        "epochs = 5000                     # Increase from 2000\n",
        "n_critic = 3                      # Reduce from 5\n",
        "lambda_gp = 5                     # Reduce fro\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize models and move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator(z_dim, input_dim).to(device)\n",
        "critic = Critic(input_dim).to(device)\n",
        "\n",
        "# Modify learning rates\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.9))  # Increased LR\n",
        "optimizer_C = optim.Adam(critic.parameters(), lr=0.00005, betas=(0.5, 0.9))    # Reduced LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUXtdkJSFrrB",
        "outputId": "e1253a4e-b538-483c-8142-e89d1631d110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000 | C_loss: 4.2372 | G_loss: -0.0180\n",
            "Epoch 200/5000 | C_loss: 0.0910 | G_loss: -0.0712\n",
            "Epoch 400/5000 | C_loss: 0.1300 | G_loss: -0.0953\n",
            "Epoch 600/5000 | C_loss: 0.0597 | G_loss: -0.1254\n",
            "Epoch 800/5000 | C_loss: 0.0652 | G_loss: -0.2060\n",
            "Epoch 1000/5000 | C_loss: 0.0640 | G_loss: -0.3185\n",
            "Epoch 1200/5000 | C_loss: 0.1638 | G_loss: -0.2992\n",
            "Epoch 1400/5000 | C_loss: 0.1020 | G_loss: -0.3393\n",
            "Epoch 1600/5000 | C_loss: 0.0869 | G_loss: -0.2788\n",
            "Epoch 1800/5000 | C_loss: 0.0355 | G_loss: -0.1588\n",
            "Epoch 2000/5000 | C_loss: 0.3004 | G_loss: -0.2085\n",
            "Epoch 2200/5000 | C_loss: 0.0659 | G_loss: -0.1926\n",
            "Epoch 2400/5000 | C_loss: 0.0768 | G_loss: -0.2686\n",
            "Epoch 2600/5000 | C_loss: 0.0613 | G_loss: -0.1996\n",
            "Epoch 2800/5000 | C_loss: 0.0829 | G_loss: -0.1548\n",
            "Epoch 3000/5000 | C_loss: 0.0886 | G_loss: -0.0949\n",
            "Epoch 3200/5000 | C_loss: 0.1414 | G_loss: -0.1225\n",
            "Epoch 3400/5000 | C_loss: 0.1149 | G_loss: -0.1209\n",
            "Epoch 3600/5000 | C_loss: 0.0798 | G_loss: -0.1602\n",
            "Epoch 3800/5000 | C_loss: 0.1267 | G_loss: -0.4577\n",
            "Epoch 4000/5000 | C_loss: 0.0686 | G_loss: -0.4880\n",
            "Epoch 4200/5000 | C_loss: 0.0839 | G_loss: -0.4663\n",
            "Epoch 4400/5000 | C_loss: 0.0416 | G_loss: -0.4510\n",
            "Epoch 4600/5000 | C_loss: 0.0708 | G_loss: -0.4377\n",
            "Epoch 4800/5000 | C_loss: 0.0504 | G_loss: -0.3793\n"
          ]
        }
      ],
      "source": [
        "# Gradient penalty for WGAN-GP\n",
        "def compute_gradient_penalty(critic, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = critic(interpolates)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Enable cuDNN auto-tuner for faster training\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        real_batch = batch[0].to(device)\n",
        "\n",
        "        # Train Critic (n_critic times)\n",
        "        for _ in range(n_critic):\n",
        "            # Generate fake data\n",
        "            z = torch.randn(real_batch.size(0), z_dim, device=device)\n",
        "            fake_batch = generator(z)\n",
        "\n",
        "            # Compute critic scores\n",
        "            real_validity = critic(real_batch)\n",
        "            fake_validity = critic(fake_batch)\n",
        "\n",
        "            # Gradient penalty\n",
        "            gradient_penalty = compute_gradient_penalty(critic, real_batch.data, fake_batch.data)\n",
        "\n",
        "            # Critic loss\n",
        "            c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "            optimizer_C.zero_grad()\n",
        "            c_loss.backward()\n",
        "            optimizer_C.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(real_batch.size(0), z_dim, device=device)\n",
        "        gen_batch = generator(z)\n",
        "        g_loss = -torch.mean(critic(gen_batch))\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}/{epochs} | C_loss: {c_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic fraud samples\n",
        "generator.eval()\n",
        "num_original_fraud_train = len(fraud_train)\n",
        "num_legit_train = len(legit_train)\n",
        "num_synthetic_needed = num_legit_train - num_original_fraud_train\n",
        "\n",
        "synthetic_samples = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Generate in batches\n",
        "    for _ in range(0, num_synthetic_needed, batch_size):\n",
        "        batch_size_ = min(batch_size, num_synthetic_needed - len(synthetic_samples))\n",
        "        z = torch.randn(batch_size_, z_dim, device=device)\n",
        "        gen_samples = generator(z).cpu().numpy()\n",
        "        synthetic_samples.extend(gen_samples)\n",
        "\n",
        "# Create synthetic fraud dataframe\n",
        "synthetic_fraud = pd.DataFrame(synthetic_samples, columns=fraud_train.drop(columns=['Class']).columns)\n",
        "synthetic_fraud['Class'] = 1\n",
        "\n",
        "# Create balanced training set\n",
        "balanced_train = pd.concat([\n",
        "    fraud_train,  # Original fraud training data\n",
        "    legit_train,   # Original legitimate training data\n",
        "    synthetic_fraud  # Synthetic fraud\n",
        "], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "balanced_train = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save datasets\n",
        "balanced_train.to_csv('/content/drive/MyDrive/Colab Notebooks/archive/balanced_creditcard_train.csv', index=False)\n",
        "final_test_set.to_csv('/content/drive/MyDrive/Colab Notebooks/archive/creditcard_test.csv', index=False)\n",
        "\n",
        "print(f\"Original fraud training samples: {len(fraud_train)}\")\n",
        "print(f\"Synthetic fraud samples generated: {len(synthetic_fraud)}\")\n",
        "print(f\"Balanced training set size: {len(balanced_train)}\")\n",
        "print(f\"Test set size: {len(final_test_set)}\")\n",
        "print(f\"Class distribution in training set:\\n{balanced_train['Class'].value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySyHie2cHudi",
        "outputId": "18eb5e47-3a47-4597-d798-26e40438659c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original fraud training samples: 344\n",
            "Synthetic fraud samples generated: 198676\n",
            "Balanced training set size: 398040\n",
            "Test set size: 85443\n",
            "Class distribution in training set:\n",
            "Class\n",
            "0    199020\n",
            "1    199020\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Original fraud training samples: {len(fraud_train)}\")\n",
        "print(f\"Synthetic fraud samples generated: {len(synthetic_fraud)}\")\n",
        "print(f\"Balanced training set size: {len(balanced_train)}\")\n",
        "print(f\"Test set size: {len(final_test_set)}\")\n",
        "print(f\"Class distribution in training set:\\n{balanced_train['Class'].value_counts()}\")\n",
        "print(f\"Class distribution in testing set:\\n{final_test_set['Class'].value_counts()}\")"
      ],
      "metadata": {
        "id": "Vv-cugiqM7_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73cef6e9-633e-4c5a-ed67-d0336da769d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original fraud training samples: 344\n",
            "Synthetic fraud samples generated: 198676\n",
            "Balanced training set size: 398040\n",
            "Test set size: 85443\n",
            "Class distribution in training set:\n",
            "Class\n",
            "0    199020\n",
            "1    199020\n",
            "Name: count, dtype: int64\n",
            "Class distribution in testing set:\n",
            "Class\n",
            "0    85295\n",
            "1      148\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}