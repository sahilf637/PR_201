{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xnQ1kK61Frqu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd5_mbySF2W6",
        "outputId": "738f7330-645d-4fd3-bd3a-be51e4ae0c2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r4Rm0RQmFrq4"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/archive/creditcard.csv')\n",
        "\n",
        "# Split data into fraud and legitimate\n",
        "fraud_data = df[df['Class'] == 1]\n",
        "legit_data = df[df['Class'] == 0]\n",
        "\n",
        "# Split both classes into train and test (80/20)\n",
        "fraud_train, fraud_test = train_test_split(fraud_data, test_size=0.2, random_state=42)\n",
        "legit_train, legit_test = train_test_split(legit_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preserve test set for final evaluation\n",
        "final_test_set = pd.concat([fraud_test, legit_test])\n",
        "\n",
        "# GAN will only train on fraud TRAINING data\n",
        "fraud_train_features = fraud_train.drop(columns=['Class']).values\n",
        "\n",
        "# Normalize data using scaler fitted ONLY on fraud training data\n",
        "scaler = MinMaxScaler()\n",
        "fraud_train_scaled = scaler.fit_transform(fraud_train_features)\n",
        "\n",
        "# Convert to PyTorch dataset\n",
        "real_data = torch.tensor(fraud_train_scaled, dtype=torch.float32)\n",
        "dataset = TensorDataset(real_data)\n",
        "data_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a2IjWjcyFrq7"
      },
      "outputs": [],
      "source": [
        "# Improved Generator with regularization\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Critic (Discriminator) for WGAN-GP\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oyt0H0XZFrq-"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "input_dim = real_data.shape[1]\n",
        "z_dim = 30\n",
        "n_critic = 5\n",
        "lambda_gp = 10\n",
        "epochs = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize models and move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator(z_dim, input_dim).to(device)\n",
        "critic = Critic(input_dim).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
        "optimizer_C = optim.Adam(critic.parameters(), lr=0.0001, betas=(0.5, 0.9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUXtdkJSFrrB",
        "outputId": "f57f7652-6c72-460b-d71f-fe3a5b84580a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2000 | C_loss: 6.8331 | G_loss: 0.0691\n",
            "Epoch 200/2000 | C_loss: 0.0990 | G_loss: -0.2411\n",
            "Epoch 400/2000 | C_loss: 0.1599 | G_loss: -0.2695\n",
            "Epoch 600/2000 | C_loss: 0.1251 | G_loss: -0.6096\n",
            "Epoch 800/2000 | C_loss: 0.0660 | G_loss: -0.6240\n",
            "Epoch 1000/2000 | C_loss: 0.0959 | G_loss: -0.5625\n",
            "Epoch 1200/2000 | C_loss: 0.0302 | G_loss: 0.5580\n",
            "Epoch 1400/2000 | C_loss: -0.0135 | G_loss: 0.6501\n",
            "Epoch 1600/2000 | C_loss: 0.1531 | G_loss: 0.5439\n",
            "Epoch 1800/2000 | C_loss: 0.0593 | G_loss: 0.5019\n"
          ]
        }
      ],
      "source": [
        "# Gradient penalty for WGAN-GP\n",
        "def compute_gradient_penalty(critic, real_samples, fake_samples):\n",
        "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = critic(interpolates)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Enable cuDNN auto-tuner for faster training\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        real_batch = batch[0].to(device)\n",
        "\n",
        "        # Train Critic (n_critic times)\n",
        "        for _ in range(n_critic):\n",
        "            # Generate fake data\n",
        "            z = torch.randn(real_batch.size(0), z_dim, device=device)\n",
        "            fake_batch = generator(z)\n",
        "\n",
        "            # Compute critic scores\n",
        "            real_validity = critic(real_batch)\n",
        "            fake_validity = critic(fake_batch)\n",
        "\n",
        "            # Gradient penalty\n",
        "            gradient_penalty = compute_gradient_penalty(critic, real_batch.data, fake_batch.data)\n",
        "\n",
        "            # Critic loss\n",
        "            c_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "            optimizer_C.zero_grad()\n",
        "            c_loss.backward()\n",
        "            optimizer_C.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(real_batch.size(0), z_dim, device=device)\n",
        "        gen_batch = generator(z)\n",
        "        g_loss = -torch.mean(critic(gen_batch))\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch}/{epochs} | C_loss: {c_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic fraud samples\n",
        "generator.eval()\n",
        "num_original_fraud_train = len(fraud_train)\n",
        "num_legit_train = len(legit_train)\n",
        "num_synthetic_needed = num_legit_train - num_original_fraud_train\n",
        "\n",
        "synthetic_samples = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Generate in batches\n",
        "    for _ in range(0, num_synthetic_needed, batch_size):\n",
        "        batch_size_ = min(batch_size, num_synthetic_needed - len(synthetic_samples))\n",
        "        z = torch.randn(batch_size_, z_dim, device=device)\n",
        "        gen_samples = generator(z).cpu().numpy()\n",
        "        synthetic_samples.extend(gen_samples)\n",
        "\n",
        "# Create synthetic fraud dataframe\n",
        "synthetic_fraud = pd.DataFrame(synthetic_samples, columns=fraud_train.drop(columns=['Class']).columns)\n",
        "synthetic_fraud['Class'] = 1\n",
        "\n",
        "# Create balanced training set\n",
        "balanced_train = pd.concat([\n",
        "    fraud_train,  # Original fraud training data\n",
        "    legit_train,   # Original legitimate training data\n",
        "    synthetic_fraud  # Synthetic fraud\n",
        "], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "balanced_train = balanced_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save datasets\n",
        "balanced_train.to_csv('/content/drive/MyDrive/Colab Notebooks/archive/balanced_creditcard_train.csv', index=False)\n",
        "final_test_set.to_csv('/content/drive/MyDrive/Colab Notebooks/archive/creditcard_test.csv', index=False)\n",
        "\n",
        "print(f\"Original fraud training samples: {len(fraud_train)}\")\n",
        "print(f\"Synthetic fraud samples generated: {len(synthetic_fraud)}\")\n",
        "print(f\"Balanced training set size: {len(balanced_train)}\")\n",
        "print(f\"Test set size: {len(final_test_set)}\")\n",
        "print(f\"Class distribution in training set:\\n{balanced_train['Class'].value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySyHie2cHudi",
        "outputId": "c2a4de90-af2e-4471-9ef4-df2dd5ccdc75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original fraud training samples: 393\n",
            "Synthetic fraud samples generated: 227059\n",
            "Balanced training set size: 454904\n",
            "Test set size: 56962\n",
            "Class distribution in training set:\n",
            "Class\n",
            "1    227452\n",
            "0    227452\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Original fraud training samples: {len(fraud_train)}\")\n",
        "print(f\"Synthetic fraud samples generated: {len(synthetic_fraud)}\")\n",
        "print(f\"Balanced training set size: {len(balanced_train)}\")\n",
        "print(f\"Test set size: {len(final_test_set)}\")\n",
        "print(f\"Class distribution in training set:\\n{balanced_train['Class'].value_counts()}\")\n",
        "print(f\"Class distribution in training set:\\n{final_test_set['Class'].value_counts()}\")"
      ],
      "metadata": {
        "id": "Vv-cugiqM7_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5711a08-fd69-4cb7-e626-851c5e42ca45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original fraud training samples: 393\n",
            "Synthetic fraud samples generated: 227059\n",
            "Balanced training set size: 454904\n",
            "Test set size: 56962\n",
            "Class distribution in training set:\n",
            "Class\n",
            "1    227452\n",
            "0    227452\n",
            "Name: count, dtype: int64\n",
            "Class distribution in training set:\n",
            "Class\n",
            "0    56863\n",
            "1       99\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def evaluate_model(model, x_train, y_train, x_legit_test, y_legit_test, x_fraud_test, y_fraud_test):\n",
        "    \"\"\"\n",
        "    Evaluates a given model on training and test data.\n",
        "\n",
        "    Args:\n",
        "        model: The machine learning model to evaluate.\n",
        "        x_train, y_train: Training data and labels.\n",
        "        x_legit_test, y_legit_test: Test data and labels for legitimate transactions.\n",
        "        x_fraud_test, y_fraud_test: Test data and labels for fraudulent transactions.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing:\n",
        "            - Legitimate accuracy\n",
        "            - Fraud accuracy\n",
        "    \"\"\"\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate accuracies\n",
        "    accuracy_legit = model.score(x_legit_test, y_legit_test)\n",
        "    accuracy_fraud = model.score(x_fraud_test, y_fraud_test)\n",
        "\n",
        "    return {\n",
        "        \"accuracy_legit\": accuracy_legit,\n",
        "        \"accuracy_fraud\": accuracy_fraud\n",
        "    }\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=4, random_state=42),\n",
        "    \"SVM\": SVC(kernel='rbf', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "XgEpEUojM4gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_fraud_test = fraud_test[:, 1:30]\n",
        "y_fraud_test = fraud_test[:, 30]\n",
        "\n",
        "x_legit_test = legit_test[:, 1:30]\n",
        "y_legit_test = legit_test[:, 30]\n",
        "\n",
        "print('X_train Shape:', x_train.shape)\n",
        "print('Y_train Shape:', y_train.shape)\n",
        "print('X_fraud_test Shape:', x_fraud_test.shape)\n",
        "print('Y_fraud_test Shape:', y_fraud_test.shape)\n",
        "print('X_legit_test Shape:', x_legit_test.shape)\n",
        "print('Y_legit_test Shape:', y_legit_test.shape)\n"
      ],
      "metadata": {
        "id": "wMtD18_VNwGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    results[model_name] = evaluate_model(model, x_train, y_train, x_legit_test, y_legit_test, x_fraud_test, y_fraud_test)\n",
        "    print(f\"{model_name} Legit Accuracy: {results[model_name]['accuracy_legit']:.5f}\")\n",
        "    print(f\"{model_name} Fraud Accuracy: {results[model_name]['accuracy_fraud']:.5f}\")\n"
      ],
      "metadata": {
        "id": "xKy3erRYO7xm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}